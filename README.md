# üöÄ Two LLM Projects Showcase
In this code i'm demonstrating the usage of Large Language Models! Both projects leverage Google's Gemini API but showcase completely different AI capabilities - one using function calling for structured interactions and the other using embeddings for semantic understanding.
# üçï Mamgo's Pizza Ordering System
The architecture uses predefined Python functions that are exposed to the LLM through JSON schema definitions, enabling the AI to execute specific business logic while maintaining conversational flow. The system implements a state management pattern with persistent order tracking, dynamic function registration for menu operations (add_pizza, remove_item, calculate_total, process_checkout), type validation and error handling for all function parameters, real-time price calculations and context-aware responses that maintain conversation history. Built with Python using Google's generative-ai library, the system defines functions like add_pizza(size, type, toppings) and calculate_total() which the LLM can call autonomously based on user input, creating a seamless bridge between natural language and structured business operations. The function calling approach ensures data integrity and predictable behavior while enabling complex multi-step transactions through conversational AI.
# üìö Book Recommendation System
This system employs text embeddings and vector similarity search to create a semantic recommendation engine. The architecture processes book data through Google's text-embedding-004 model, converting textual descriptions into high-dimensional vectors (embeddings) that capture semantic meaning. Key technical components include batch embedding generation with API rate limiting and retry logic, vector storage and caching using NumPy arrays for persistence, cosine similarity calculations via scikit-learn for semantic matching, query preprocessing with regex-based intent parsing for genre/rating filters, and similarity ranking algorithms that sort results by relevance scores. The system implements asynchronous processing patterns for handling large datasets (13k+ books), memory-efficient caching with .npy file serialization, semantic query understanding that maps natural language to vector space, and multi-criteria filtering combining embedding similarity with metadata filters. Built with pandas, NumPy, and scikit-learn, the system creates comprehensive text representations (f"Title: {title} | Author: {author} | Genre: {genre} | Description: {desc}"), generates 768-dimensional embeddings, and uses cosine_similarity([user_embedding], [book_embeddings]) to find semantically similar content, demonstrating advanced information retrieval and semantic search capabilities.
